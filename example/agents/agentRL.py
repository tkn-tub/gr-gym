import random
import gymnasium as gym
import math
import pickle
import numpy as np
from collections import deque
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

from utils import float_to_bin_vec, bin_vec_to_float

class DQNARQSolver():
    def __init__(self, just_replay=False, n_episodes=1000, max_env_steps=100, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.999999, alpha=0.01, alpha_decay=0.01, batch_size=256, quiet=False):
        self.memory = deque(maxlen=100000)
        self.env = gym.make('grgym:grenv-v0')
        self.just_replay = just_replay
        self.n_win_ticks = 100
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_log_decay
        self.alpha = alpha
        self.alpha_decay = alpha_decay
        self.n_episodes = n_episodes
        self.batch_size = batch_size
        self.quiet = quiet
        self.max_env_steps = max_env_steps

        self.logfile = './rl/agentRLng.csv'
        with open(self.logfile, 'w') as fd:
            fd.write("\n")

        self.model_path = './rl/gnugym_rl_wifi/'

        # quantization
        self.no_bits = 6 #10
        print("No. of quantization bits: %d" % (self.no_bits))

        self.obs_high = 35
        self.obs_low = 8

        # Init model
        self.model = Sequential()
        self.model.add(Dense(self.no_bits, input_dim=self.no_bits, activation='relu'))
        self.model.add(Dense(128, activation='relu'))
        #self.model.add(Dense(32, activation='relu'))
        self.model.add(Dense(self.env.action_space.n, activation='softmax'))
        self.model.compile(loss='mse', optimizer=Adam(lr=self.alpha, decay=self.alpha_decay))

        print(self.model.summary())

        self.av_bitrates = [
            3,  # Mbps BPSK 1/2
            4.5,  # Mbps BPSK 3/4
            6,  # Mbps QPSK 1/2
            9,  # Mbps QPSK 3/4
            12,  # Mbps 16QAM 1/2
            18,  # Mbps 16QAM 3/4
            24,  # Mbps 64QAM 2/3
            27  # Mbps 64QAM 3/4
        ]
        np.seterr(invalid='raise')
        self.save_mem = True
        self.mem_fname = './rl/agentRLng.p'
        self.take_future_reward = False

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
        if self.save_mem:
            pickle.dump(self.memory, open(self.mem_fname, "wb"))

    def choose_action(self, state, epsilon):
        #return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.model.predict(state))
        valid_mcs = [3, 4, 5, 7]

        while True:
            action = self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.model.predict(state))
            if action in valid_mcs:
                return action

    def get_epsilon(self, t):
        e_val = max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))
        print("Curr eps: %.4f" % e_val)
        return e_val

    def preprocess_state(self, state):
        sc_min = 6
        sc_dc = 32
        sc_max = 59

        # remove null carriers
        obsl = state[sc_min:sc_dc]
        obsr = state[sc_dc + 1:sc_max]

        obs = []
        obs.extend(obsl)
        obs.extend(obsr)

        avg_obs = np.mean(obs)
        if avg_obs > self.obs_high or avg_obs < self.obs_low:
            print("WARNING: observation out of range! %.2f" % (avg_obs))
            avg_obs = max(avg_obs, self.obs_low)
            avg_obs = min(avg_obs, self.obs_high)

        x = (avg_obs - self.obs_low) / (self.obs_high - self.obs_low)
        return float_to_bin_vec(x, self.no_bits)

    def replay(self, batch_size):
        print("replaying ... mem sz: %d, batch sz: %d" % (len(self.memory), batch_size))
        x_batch, y_batch = [], []
        minibatch = random.sample(
            self.memory, min(len(self.memory), batch_size))
        for state, action, reward, next_state, done in minibatch:
            y_target = self.model.predict(state)
            for a in range(8):
                #if a not in valid_mcs:
                y_target[0][a] = 0

            if self.take_future_reward:
                y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])
            else:
                y_target[0][action] = reward
            x_batch.append(state[0])
            y_batch.append(y_target[0])

        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def run(self):
        scores = deque(maxlen=100)

        if self.just_replay:
            self.memory = pickle.load(open(self.mem_fname, "rb"))

        for e in range(self.n_episodes):

            if self.just_replay:
                print('nop')
            else:
                state = self.preprocess_state(self.env.reset())

                s = 0
                done = False
                i = 0

                e_val = self.get_epsilon(e)
                while not done:
                    action = self.choose_action(state, e_val)

                    next_state, reward, done, _ = self.env.step(action)
                    next_state = self.preprocess_state(next_state)

                    state_norm = bin_vec_to_float(state, self.no_bits)
                    next_state_norm = bin_vec_to_float(next_state, self.no_bits)
                    #print("%d.%d: observation: %.2f / %.2f, action: %d, reward: %.2f" % (e, s, state_norm, next_state_norm, action, reward))

                    with open(self.logfile, 'a') as fd:
                        fd.write(str(state_norm) + "," + str(action) + "," + str(reward) + "\n")

                    self.remember(state, action, reward, next_state, done)
                    state = next_state
                    i += reward
                    s += 1

                    if s > self.max_env_steps:
                        #print("Max. no. of steps reached. %d" % (s))
                        break

                #print('sum-reward last episod: %d' % (i))
                scores.append(i)
                mean_score = np.mean(scores)

                self.model.save(self.model_path)

                if not self.quiet:
                    print('[Episode {}] - Mean reward last episodes was {}.'.format(e, mean_score))

            self.replay(self.batch_size)

        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))
        return e

if __name__ == '__main__':
    agent = DQNARQSolver(just_replay=False)
    agent.run()
