{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('agentdata/rssi_sl_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset of best action for each observation (obs and label)\n",
    "cols = []\n",
    "for i in range(64):\n",
    "    cols.append('Sc' + str(i))\n",
    "\n",
    "df['Run'] = df.apply(lambda row: int(row.name / 8), axis=1)\n",
    "df = df[df['Reward'] > 0.0]\n",
    "\n",
    "obs = []\n",
    "label = []\n",
    "\n",
    "for run in df['Run'].unique():\n",
    "    runData = df[df['Run'] == run]\n",
    "    myid = runData['Reward'].idxmax()\n",
    "    myrow = df.loc[myid]\n",
    "    label.append(myrow['Action'])\n",
    "    obs.append(myrow[cols].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.10045662,  0.20091324,  0.30136986,  0.40182648,\n",
       "        0.50228311,  0.60273973,  0.70319635,  0.80365297,  0.90410959,\n",
       "        1.00456621,  1.10502283,  1.20547945,  1.30593607,  1.40639269,\n",
       "        1.50684932,  1.60730594,  1.70776256,  1.80821918,  1.9086758 ,\n",
       "        2.00913242,  2.10958904,  2.21004566,  2.31050228,  2.4109589 ,\n",
       "        2.51141553,  2.61187215,  2.71232877,  2.81278539,  2.91324201,\n",
       "        3.01369863,  3.11415525,  3.21461187,  3.31506849,  3.41552511,\n",
       "        3.51598174,  3.61643836,  3.71689498,  3.8173516 ,  3.91780822,\n",
       "        4.01826484,  4.11872146,  4.21917808,  4.3196347 ,  4.42009132,\n",
       "        4.52054795,  4.62100457,  4.72146119,  4.82191781,  4.92237443,\n",
       "        5.02283105,  5.12328767,  5.22374429,  5.32420091,  5.42465753,\n",
       "        5.52511416,  5.62557078,  5.7260274 ,  5.82648402,  5.92694064,\n",
       "        6.02739726,  6.12785388,  6.2283105 ,  6.32876712,  6.42922374,\n",
       "        6.52968037,  6.63013699,  6.73059361,  6.83105023,  6.93150685,\n",
       "        7.03196347,  7.13242009,  7.23287671,  7.33333333,  7.43378995,\n",
       "        7.53424658,  7.6347032 ,  7.73515982,  7.83561644,  7.93607306,\n",
       "        8.03652968,  8.1369863 ,  8.23744292,  8.33789954,  8.43835616,\n",
       "        8.53881279,  8.63926941,  8.73972603,  8.84018265,  8.94063927,\n",
       "        9.04109589,  9.14155251,  9.24200913,  9.34246575,  9.44292237,\n",
       "        9.543379  ,  9.64383562,  9.74429224,  9.84474886,  9.94520548,\n",
       "       10.0456621 , 10.14611872, 10.24657534, 10.34703196, 10.44748858,\n",
       "       10.54794521, 10.64840183, 10.74885845, 10.84931507, 10.94977169,\n",
       "       11.05022831, 11.15068493, 11.25114155, 11.35159817, 11.45205479,\n",
       "       11.55251142, 11.65296804, 11.75342466, 11.85388128, 11.9543379 ,\n",
       "       12.05479452, 12.15525114, 12.25570776, 12.35616438, 12.456621  ,\n",
       "       12.55707763, 12.65753425, 12.75799087, 12.85844749, 12.95890411,\n",
       "       13.05936073, 13.15981735, 13.26027397, 13.36073059, 13.46118721,\n",
       "       13.56164384, 13.66210046, 13.76255708, 13.8630137 , 13.96347032,\n",
       "       14.06392694, 14.16438356, 14.26484018, 14.3652968 , 14.46575342,\n",
       "       14.56621005, 14.66666667, 14.76712329, 14.86757991, 14.96803653,\n",
       "       15.06849315, 15.16894977, 15.26940639, 15.36986301, 15.47031963,\n",
       "       15.57077626, 15.67123288, 15.7716895 , 15.87214612, 15.97260274,\n",
       "       16.07305936, 16.17351598, 16.2739726 , 16.37442922, 16.47488584,\n",
       "       16.57534247, 16.67579909, 16.77625571, 16.87671233, 16.97716895,\n",
       "       17.07762557, 17.17808219, 17.27853881, 17.37899543, 17.47945205,\n",
       "       17.57990868, 17.6803653 , 17.78082192, 17.88127854, 17.98173516,\n",
       "       18.08219178, 18.1826484 , 18.28310502, 18.38356164, 18.48401826,\n",
       "       18.58447489, 18.68493151, 18.78538813, 18.88584475, 18.98630137,\n",
       "       19.08675799, 19.18721461, 19.28767123, 19.38812785, 19.48858447,\n",
       "       19.5890411 , 19.68949772, 19.78995434, 19.89041096, 19.99086758,\n",
       "       20.0913242 , 20.19178082, 20.29223744, 20.39269406, 20.49315068,\n",
       "       20.59360731, 20.69406393, 20.79452055, 20.89497717, 20.99543379,\n",
       "       21.09589041, 21.19634703, 21.29680365, 21.39726027, 21.49771689,\n",
       "       21.59817352, 21.69863014, 21.89954338, 22.        , 21.79908676])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Dist'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_size = 8\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(1, input_shape=(1,), activation='sigmoid'))\n",
    "model.add(keras.layers.Dense(a_size, activation='relu'))\n",
    "#model.add(keras.layers.Dense(a_size, activation='relu'))\n",
    "model.add(keras.layers.Dense(a_size, activation='softmax'))\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take average observation over all 64 subcarriers\n",
    "data = []\n",
    "for elem in obs:\n",
    "    elem = np.average(elem)\n",
    "    elem = (elem - 0) / 45 \n",
    "    elem = np.reshape(elem, [1, ])\n",
    "    data.append(elem)\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Action')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data, label, '.', alpha = 0.1)\n",
    "plt.xlabel('Observation (Average)')\n",
    "plt.ylabel('Action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6594"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomize data\n",
    "new_data = data\n",
    "new_label =[]\n",
    "new_label.extend(label)\n",
    "\n",
    "for i in range(5):\n",
    "    new_data = np.append(new_data, data + np.random.normal(0,0.005,len(data)).reshape(len(data),1))\n",
    "    new_label.extend(label)\n",
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5275 samples, validate on 1319 samples\n",
      "Epoch 1/1000\n",
      "5275/5275 [==============================] - 1s 124us/sample - loss: 1.9834 - acc: 0.2292 - val_loss: 1.9480 - val_acc: 0.2578\n",
      "Epoch 2/1000\n",
      "5275/5275 [==============================] - 0s 73us/sample - loss: 1.9311 - acc: 0.2620 - val_loss: 1.9230 - val_acc: 0.2578\n",
      "Epoch 3/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 1.9154 - acc: 0.2620 - val_loss: 1.9128 - val_acc: 0.2578\n",
      "Epoch 4/1000\n",
      "5275/5275 [==============================] - 0s 77us/sample - loss: 1.9081 - acc: 0.2620 - val_loss: 1.9073 - val_acc: 0.2578\n",
      "Epoch 5/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.9046 - acc: 0.2620 - val_loss: 1.9047 - val_acc: 0.2578\n",
      "Epoch 6/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 1.9025 - acc: 0.2620 - val_loss: 1.9032 - val_acc: 0.2578\n",
      "Epoch 7/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 1.9012 - acc: 0.2620 - val_loss: 1.9018 - val_acc: 0.2578\n",
      "Epoch 8/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.9002 - acc: 0.2620 - val_loss: 1.9003 - val_acc: 0.2578\n",
      "Epoch 9/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 1.8991 - acc: 0.2620 - val_loss: 1.8998 - val_acc: 0.2578\n",
      "Epoch 10/1000\n",
      "5275/5275 [==============================] - 0s 76us/sample - loss: 1.8972 - acc: 0.2620 - val_loss: 1.8987 - val_acc: 0.2578\n",
      "Epoch 11/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 1.8965 - acc: 0.2620 - val_loss: 1.8969 - val_acc: 0.2578\n",
      "Epoch 12/1000\n",
      "5275/5275 [==============================] - 0s 80us/sample - loss: 1.8950 - acc: 0.2620 - val_loss: 1.8953 - val_acc: 0.2578\n",
      "Epoch 13/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.8932 - acc: 0.2620 - val_loss: 1.8934 - val_acc: 0.2578\n",
      "Epoch 14/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 1.8911 - acc: 0.2620 - val_loss: 1.8912 - val_acc: 0.2578\n",
      "Epoch 15/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 1.8891 - acc: 0.2620 - val_loss: 1.8887 - val_acc: 0.2578\n",
      "Epoch 16/1000\n",
      "5275/5275 [==============================] - 0s 76us/sample - loss: 1.8864 - acc: 0.2620 - val_loss: 1.8859 - val_acc: 0.2578\n",
      "Epoch 17/1000\n",
      "5275/5275 [==============================] - 0s 78us/sample - loss: 1.8832 - acc: 0.2620 - val_loss: 1.8822 - val_acc: 0.2578\n",
      "Epoch 18/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 1.8790 - acc: 0.2620 - val_loss: 1.8783 - val_acc: 0.2578\n",
      "Epoch 19/1000\n",
      "5275/5275 [==============================] - 0s 87us/sample - loss: 1.8741 - acc: 0.2620 - val_loss: 1.8723 - val_acc: 0.2578\n",
      "Epoch 20/1000\n",
      "5275/5275 [==============================] - 0s 77us/sample - loss: 1.8678 - acc: 0.2620 - val_loss: 1.8656 - val_acc: 0.2578\n",
      "Epoch 21/1000\n",
      "5275/5275 [==============================] - 0s 88us/sample - loss: 1.8605 - acc: 0.2620 - val_loss: 1.8569 - val_acc: 0.2578\n",
      "Epoch 22/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 1.8505 - acc: 0.2620 - val_loss: 1.8460 - val_acc: 0.2578\n",
      "Epoch 23/1000\n",
      "5275/5275 [==============================] - 0s 78us/sample - loss: 1.8383 - acc: 0.2620 - val_loss: 1.8322 - val_acc: 0.2578\n",
      "Epoch 24/1000\n",
      "5275/5275 [==============================] - 0s 78us/sample - loss: 1.8231 - acc: 0.2620 - val_loss: 1.8149 - val_acc: 0.2578\n",
      "Epoch 25/1000\n",
      "5275/5275 [==============================] - 0s 83us/sample - loss: 1.8042 - acc: 0.2627 - val_loss: 1.7937 - val_acc: 0.2585\n",
      "Epoch 26/1000\n",
      "5275/5275 [==============================] - 0s 73us/sample - loss: 1.7812 - acc: 0.2618 - val_loss: 1.7698 - val_acc: 0.2593\n",
      "Epoch 27/1000\n",
      "5275/5275 [==============================] - 0s 86us/sample - loss: 1.7551 - acc: 0.2639 - val_loss: 1.7420 - val_acc: 0.2593\n",
      "Epoch 28/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 1.7257 - acc: 0.2690 - val_loss: 1.7110 - val_acc: 0.2896\n",
      "Epoch 29/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 1.6930 - acc: 0.3090 - val_loss: 1.6771 - val_acc: 0.3313\n",
      "Epoch 30/1000\n",
      "5275/5275 [==============================] - 0s 86us/sample - loss: 1.6585 - acc: 0.3456 - val_loss: 1.6415 - val_acc: 0.3692\n",
      "Epoch 31/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 1.6227 - acc: 0.3852 - val_loss: 1.6056 - val_acc: 0.3867\n",
      "Epoch 32/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 1.5860 - acc: 0.3981 - val_loss: 1.5689 - val_acc: 0.3920\n",
      "Epoch 33/1000\n",
      "5275/5275 [==============================] - 0s 76us/sample - loss: 1.5496 - acc: 0.4044 - val_loss: 1.5329 - val_acc: 0.3980\n",
      "Epoch 34/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 1.5144 - acc: 0.4076 - val_loss: 1.4990 - val_acc: 0.3995\n",
      "Epoch 35/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.4814 - acc: 0.4089 - val_loss: 1.4687 - val_acc: 0.4011\n",
      "Epoch 36/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 1.4513 - acc: 0.4227 - val_loss: 1.4391 - val_acc: 0.4230\n",
      "Epoch 37/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 1.4239 - acc: 0.4402 - val_loss: 1.4131 - val_acc: 0.4329\n",
      "Epoch 38/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.3997 - acc: 0.4504 - val_loss: 1.3896 - val_acc: 0.4412\n",
      "Epoch 39/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.3774 - acc: 0.4584 - val_loss: 1.3689 - val_acc: 0.4511\n",
      "Epoch 40/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.3569 - acc: 0.4664 - val_loss: 1.3492 - val_acc: 0.4579\n",
      "Epoch 41/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.3378 - acc: 0.4724 - val_loss: 1.3307 - val_acc: 0.4594\n",
      "Epoch 42/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.3198 - acc: 0.4756 - val_loss: 1.3127 - val_acc: 0.4655\n",
      "Epoch 43/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.3029 - acc: 0.4811 - val_loss: 1.2963 - val_acc: 0.4701\n",
      "Epoch 44/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.2869 - acc: 0.4929 - val_loss: 1.2814 - val_acc: 0.4754\n",
      "Epoch 45/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 1.2717 - acc: 0.4967 - val_loss: 1.2651 - val_acc: 0.4875\n",
      "Epoch 46/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 1.2571 - acc: 0.4978 - val_loss: 1.2514 - val_acc: 0.5110\n",
      "Epoch 47/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.2435 - acc: 0.5225 - val_loss: 1.2381 - val_acc: 0.5042\n",
      "Epoch 48/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.2300 - acc: 0.5196 - val_loss: 1.2249 - val_acc: 0.5337\n",
      "Epoch 49/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.2170 - acc: 0.5312 - val_loss: 1.2122 - val_acc: 0.5178\n",
      "Epoch 50/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.2048 - acc: 0.5371 - val_loss: 1.1996 - val_acc: 0.5633\n",
      "Epoch 51/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.1924 - acc: 0.5653 - val_loss: 1.1883 - val_acc: 0.5155\n",
      "Epoch 52/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 1.1810 - acc: 0.5492 - val_loss: 1.1769 - val_acc: 0.5421\n",
      "Epoch 53/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.1697 - acc: 0.5522 - val_loss: 1.1668 - val_acc: 0.5398\n",
      "Epoch 54/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.1585 - acc: 0.5647 - val_loss: 1.1543 - val_acc: 0.5777\n",
      "Epoch 55/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.1480 - acc: 0.5750 - val_loss: 1.1445 - val_acc: 0.5770\n",
      "Epoch 56/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.1375 - acc: 0.5846 - val_loss: 1.1345 - val_acc: 0.5519\n",
      "Epoch 57/1000\n",
      "5275/5275 [==============================] - 0s 64us/sample - loss: 1.1270 - acc: 0.5742 - val_loss: 1.1237 - val_acc: 0.5861\n",
      "Epoch 58/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.1170 - acc: 0.5882 - val_loss: 1.1138 - val_acc: 0.5906\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.1073 - acc: 0.5956 - val_loss: 1.1065 - val_acc: 0.5557\n",
      "Epoch 60/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 1.0975 - acc: 0.5879 - val_loss: 1.0938 - val_acc: 0.5997\n",
      "Epoch 61/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 1.0881 - acc: 0.5989 - val_loss: 1.0850 - val_acc: 0.5944\n",
      "Epoch 62/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 1.0789 - acc: 0.6051 - val_loss: 1.0757 - val_acc: 0.5997\n",
      "Epoch 63/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 1.0698 - acc: 0.6013 - val_loss: 1.0669 - val_acc: 0.6133\n",
      "Epoch 64/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.0611 - acc: 0.6142 - val_loss: 1.0608 - val_acc: 0.5732\n",
      "Epoch 65/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 1.0529 - acc: 0.6097 - val_loss: 1.0516 - val_acc: 0.6020\n",
      "Epoch 66/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 1.0446 - acc: 0.6135 - val_loss: 1.0430 - val_acc: 0.6088\n",
      "Epoch 67/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.0364 - acc: 0.6150 - val_loss: 1.0355 - val_acc: 0.6171\n",
      "Epoch 68/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 1.0291 - acc: 0.6239 - val_loss: 1.0266 - val_acc: 0.6187\n",
      "Epoch 69/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.0211 - acc: 0.6222 - val_loss: 1.0215 - val_acc: 0.6187\n",
      "Epoch 70/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.0139 - acc: 0.6245 - val_loss: 1.0119 - val_acc: 0.6300\n",
      "Epoch 71/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 1.0070 - acc: 0.6298 - val_loss: 1.0078 - val_acc: 0.6141\n",
      "Epoch 72/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.0006 - acc: 0.6315 - val_loss: 1.0015 - val_acc: 0.6103\n",
      "Epoch 73/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.9936 - acc: 0.6290 - val_loss: 0.9948 - val_acc: 0.6277\n",
      "Epoch 74/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.9872 - acc: 0.6332 - val_loss: 0.9887 - val_acc: 0.6194\n",
      "Epoch 75/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 0.9810 - acc: 0.6337 - val_loss: 0.9828 - val_acc: 0.6164\n",
      "Epoch 76/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.9749 - acc: 0.6311 - val_loss: 0.9736 - val_acc: 0.6376\n",
      "Epoch 77/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.9687 - acc: 0.6360 - val_loss: 0.9688 - val_acc: 0.6315\n",
      "Epoch 78/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.9629 - acc: 0.6377 - val_loss: 0.9637 - val_acc: 0.6270\n",
      "Epoch 79/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.9578 - acc: 0.6337 - val_loss: 0.9574 - val_acc: 0.6353\n",
      "Epoch 80/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.9519 - acc: 0.6398 - val_loss: 0.9514 - val_acc: 0.6429\n",
      "Epoch 81/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.9465 - acc: 0.6349 - val_loss: 0.9490 - val_acc: 0.6315\n",
      "Epoch 82/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.9412 - acc: 0.6383 - val_loss: 0.9411 - val_acc: 0.6444\n",
      "Epoch 83/1000\n",
      "5275/5275 [==============================] - 0s 73us/sample - loss: 0.9363 - acc: 0.6398 - val_loss: 0.9363 - val_acc: 0.6406\n",
      "Epoch 84/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.9316 - acc: 0.6425 - val_loss: 0.9322 - val_acc: 0.6391\n",
      "Epoch 85/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.9267 - acc: 0.6398 - val_loss: 0.9267 - val_acc: 0.6444\n",
      "Epoch 86/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.9221 - acc: 0.6451 - val_loss: 0.9243 - val_acc: 0.6361\n",
      "Epoch 87/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.9172 - acc: 0.6447 - val_loss: 0.9177 - val_acc: 0.6755\n",
      "Epoch 88/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.9130 - acc: 0.6601 - val_loss: 0.9142 - val_acc: 0.6657\n",
      "Epoch 89/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.9091 - acc: 0.6701 - val_loss: 0.9105 - val_acc: 0.6717\n",
      "Epoch 90/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.9046 - acc: 0.6745 - val_loss: 0.9055 - val_acc: 0.6846\n",
      "Epoch 91/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.9009 - acc: 0.6777 - val_loss: 0.9024 - val_acc: 0.6793\n",
      "Epoch 92/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.8966 - acc: 0.6773 - val_loss: 0.8983 - val_acc: 0.6854\n",
      "Epoch 93/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8930 - acc: 0.6808 - val_loss: 0.8953 - val_acc: 0.6801\n",
      "Epoch 94/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.8889 - acc: 0.6789 - val_loss: 0.8910 - val_acc: 0.6839\n",
      "Epoch 95/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8849 - acc: 0.6819 - val_loss: 0.8888 - val_acc: 0.6785\n",
      "Epoch 96/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.8816 - acc: 0.6828 - val_loss: 0.8835 - val_acc: 0.6876\n",
      "Epoch 97/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8782 - acc: 0.6859 - val_loss: 0.8832 - val_acc: 0.6816\n",
      "Epoch 98/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.8745 - acc: 0.6825 - val_loss: 0.8766 - val_acc: 0.6892\n",
      "Epoch 99/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.8714 - acc: 0.6845 - val_loss: 0.8736 - val_acc: 0.6869\n",
      "Epoch 100/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.8682 - acc: 0.6840 - val_loss: 0.8711 - val_acc: 0.6876\n",
      "Epoch 101/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8650 - acc: 0.6859 - val_loss: 0.8705 - val_acc: 0.6785\n",
      "Epoch 102/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8616 - acc: 0.6863 - val_loss: 0.8671 - val_acc: 0.6808\n",
      "Epoch 103/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8590 - acc: 0.6851 - val_loss: 0.8634 - val_acc: 0.6899\n",
      "Epoch 104/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.8558 - acc: 0.6874 - val_loss: 0.8587 - val_acc: 0.6937\n",
      "Epoch 105/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.8530 - acc: 0.6900 - val_loss: 0.8600 - val_acc: 0.6801\n",
      "Epoch 106/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.8503 - acc: 0.6906 - val_loss: 0.8529 - val_acc: 0.6884\n",
      "Epoch 107/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8473 - acc: 0.6902 - val_loss: 0.8514 - val_acc: 0.6952\n",
      "Epoch 108/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.8447 - acc: 0.6912 - val_loss: 0.8506 - val_acc: 0.6839\n",
      "Epoch 109/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8422 - acc: 0.6910 - val_loss: 0.8474 - val_acc: 0.6884\n",
      "Epoch 110/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8397 - acc: 0.6904 - val_loss: 0.8431 - val_acc: 0.6892\n",
      "Epoch 111/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.8371 - acc: 0.6887 - val_loss: 0.8407 - val_acc: 0.6892\n",
      "Epoch 112/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.8345 - acc: 0.6912 - val_loss: 0.8418 - val_acc: 0.6823\n",
      "Epoch 113/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.8326 - acc: 0.6897 - val_loss: 0.8379 - val_acc: 0.6998\n",
      "Epoch 114/1000\n",
      "5275/5275 [==============================] - 0s 64us/sample - loss: 0.8303 - acc: 0.6954 - val_loss: 0.8358 - val_acc: 0.6937\n",
      "Epoch 115/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.8279 - acc: 0.6925 - val_loss: 0.8366 - val_acc: 0.6899\n",
      "Epoch 116/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8264 - acc: 0.6938 - val_loss: 0.8350 - val_acc: 0.6861\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.8239 - acc: 0.6955 - val_loss: 0.8274 - val_acc: 0.7051\n",
      "Epoch 118/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.8214 - acc: 0.6959 - val_loss: 0.8268 - val_acc: 0.6899\n",
      "Epoch 119/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.8200 - acc: 0.6942 - val_loss: 0.8239 - val_acc: 0.6983\n",
      "Epoch 120/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.8177 - acc: 0.6973 - val_loss: 0.8226 - val_acc: 0.6922\n",
      "Epoch 121/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.8159 - acc: 0.6976 - val_loss: 0.8205 - val_acc: 0.7005\n",
      "Epoch 122/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8138 - acc: 0.6963 - val_loss: 0.8192 - val_acc: 0.6952\n",
      "Epoch 123/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.8123 - acc: 0.6936 - val_loss: 0.8160 - val_acc: 0.6998\n",
      "Epoch 124/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.8104 - acc: 0.6967 - val_loss: 0.8150 - val_acc: 0.7020\n",
      "Epoch 125/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.8082 - acc: 0.6999 - val_loss: 0.8122 - val_acc: 0.6960\n",
      "Epoch 126/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8071 - acc: 0.6952 - val_loss: 0.8127 - val_acc: 0.7081\n",
      "Epoch 127/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.8054 - acc: 0.6980 - val_loss: 0.8133 - val_acc: 0.6975\n",
      "Epoch 128/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.8040 - acc: 0.6963 - val_loss: 0.8112 - val_acc: 0.7036\n",
      "Epoch 129/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.8017 - acc: 0.7001 - val_loss: 0.8064 - val_acc: 0.6975\n",
      "Epoch 130/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.8003 - acc: 0.6984 - val_loss: 0.8097 - val_acc: 0.6975\n",
      "Epoch 131/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7990 - acc: 0.6995 - val_loss: 0.8066 - val_acc: 0.6937\n",
      "Epoch 132/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7977 - acc: 0.7007 - val_loss: 0.8084 - val_acc: 0.6952\n",
      "Epoch 133/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7967 - acc: 0.6978 - val_loss: 0.8027 - val_acc: 0.7013\n",
      "Epoch 134/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7944 - acc: 0.7001 - val_loss: 0.7993 - val_acc: 0.6990\n",
      "Epoch 135/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7933 - acc: 0.6976 - val_loss: 0.7974 - val_acc: 0.7074\n",
      "Epoch 136/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7919 - acc: 0.6988 - val_loss: 0.7969 - val_acc: 0.7058\n",
      "Epoch 137/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7906 - acc: 0.6988 - val_loss: 0.7954 - val_acc: 0.6983\n",
      "Epoch 138/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7893 - acc: 0.7014 - val_loss: 0.7957 - val_acc: 0.7058\n",
      "Epoch 139/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7875 - acc: 0.7007 - val_loss: 0.7925 - val_acc: 0.6929\n",
      "Epoch 140/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7867 - acc: 0.6990 - val_loss: 0.7924 - val_acc: 0.7013\n",
      "Epoch 141/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7857 - acc: 0.6993 - val_loss: 0.7950 - val_acc: 0.6937\n",
      "Epoch 142/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7851 - acc: 0.7012 - val_loss: 0.7914 - val_acc: 0.7066\n",
      "Epoch 143/1000\n",
      "5275/5275 [==============================] - 0s 73us/sample - loss: 0.7833 - acc: 0.7037 - val_loss: 0.7891 - val_acc: 0.7089\n",
      "Epoch 144/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7819 - acc: 0.7020 - val_loss: 0.7885 - val_acc: 0.7020\n",
      "Epoch 145/1000\n",
      "5275/5275 [==============================] - 0s 82us/sample - loss: 0.7809 - acc: 0.7018 - val_loss: 0.7865 - val_acc: 0.7005\n",
      "Epoch 146/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7798 - acc: 0.7007 - val_loss: 0.7863 - val_acc: 0.7058\n",
      "Epoch 147/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7788 - acc: 0.7007 - val_loss: 0.7831 - val_acc: 0.7043\n",
      "Epoch 148/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7776 - acc: 0.6986 - val_loss: 0.7819 - val_acc: 0.6998\n",
      "Epoch 149/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7769 - acc: 0.7041 - val_loss: 0.7828 - val_acc: 0.7058\n",
      "Epoch 150/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.7760 - acc: 0.7003 - val_loss: 0.7825 - val_acc: 0.7036\n",
      "Epoch 151/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7744 - acc: 0.7010 - val_loss: 0.7810 - val_acc: 0.7043\n",
      "Epoch 152/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7737 - acc: 0.7029 - val_loss: 0.7815 - val_acc: 0.7081\n",
      "Epoch 153/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7724 - acc: 0.7056 - val_loss: 0.7805 - val_acc: 0.7089\n",
      "Epoch 154/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7717 - acc: 0.7031 - val_loss: 0.7797 - val_acc: 0.7028\n",
      "Epoch 155/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7709 - acc: 0.6993 - val_loss: 0.7763 - val_acc: 0.7043\n",
      "Epoch 156/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7699 - acc: 0.7035 - val_loss: 0.7789 - val_acc: 0.6983\n",
      "Epoch 157/1000\n",
      "5275/5275 [==============================] - 0s 64us/sample - loss: 0.7694 - acc: 0.6988 - val_loss: 0.7747 - val_acc: 0.7036\n",
      "Epoch 158/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7680 - acc: 0.7027 - val_loss: 0.7742 - val_acc: 0.7036\n",
      "Epoch 159/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.7674 - acc: 0.7016 - val_loss: 0.7720 - val_acc: 0.7028\n",
      "Epoch 160/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7669 - acc: 0.7022 - val_loss: 0.7715 - val_acc: 0.7036\n",
      "Epoch 161/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7661 - acc: 0.7003 - val_loss: 0.7739 - val_acc: 0.7005\n",
      "Epoch 162/1000\n",
      "5275/5275 [==============================] - 0s 64us/sample - loss: 0.7650 - acc: 0.7039 - val_loss: 0.7740 - val_acc: 0.6952\n",
      "Epoch 163/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7643 - acc: 0.7022 - val_loss: 0.7707 - val_acc: 0.6998\n",
      "Epoch 164/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7634 - acc: 0.7035 - val_loss: 0.7691 - val_acc: 0.7013\n",
      "Epoch 165/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7624 - acc: 0.7043 - val_loss: 0.7694 - val_acc: 0.7043\n",
      "Epoch 166/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7616 - acc: 0.7014 - val_loss: 0.7682 - val_acc: 0.7028\n",
      "Epoch 167/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7612 - acc: 0.7033 - val_loss: 0.7676 - val_acc: 0.7066\n",
      "Epoch 168/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7604 - acc: 0.7020 - val_loss: 0.7652 - val_acc: 0.7013\n",
      "Epoch 169/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7599 - acc: 0.7054 - val_loss: 0.7687 - val_acc: 0.6945\n",
      "Epoch 170/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7586 - acc: 0.7041 - val_loss: 0.7641 - val_acc: 0.7036\n",
      "Epoch 171/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7586 - acc: 0.7054 - val_loss: 0.7678 - val_acc: 0.7028\n",
      "Epoch 172/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.7575 - acc: 0.7037 - val_loss: 0.7629 - val_acc: 0.7043\n",
      "Epoch 173/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7573 - acc: 0.7045 - val_loss: 0.7656 - val_acc: 0.7005\n",
      "Epoch 174/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7562 - acc: 0.7067 - val_loss: 0.7643 - val_acc: 0.7005\n",
      "Epoch 175/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7558 - acc: 0.7056 - val_loss: 0.7633 - val_acc: 0.7051\n",
      "Epoch 176/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7550 - acc: 0.7056 - val_loss: 0.7653 - val_acc: 0.6975\n",
      "Epoch 177/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7544 - acc: 0.7048 - val_loss: 0.7640 - val_acc: 0.7043\n",
      "Epoch 178/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7538 - acc: 0.7060 - val_loss: 0.7590 - val_acc: 0.7051\n",
      "Epoch 179/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7534 - acc: 0.7045 - val_loss: 0.7642 - val_acc: 0.7058\n",
      "Epoch 180/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7529 - acc: 0.7014 - val_loss: 0.7590 - val_acc: 0.7036\n",
      "Epoch 181/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7523 - acc: 0.7060 - val_loss: 0.7582 - val_acc: 0.7028\n",
      "Epoch 182/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7510 - acc: 0.7050 - val_loss: 0.7632 - val_acc: 0.7036\n",
      "Epoch 183/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7508 - acc: 0.7039 - val_loss: 0.7604 - val_acc: 0.7013\n",
      "Epoch 184/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7508 - acc: 0.7062 - val_loss: 0.7600 - val_acc: 0.6998\n",
      "Epoch 185/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7504 - acc: 0.7024 - val_loss: 0.7555 - val_acc: 0.7020\n",
      "Epoch 186/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7492 - acc: 0.7045 - val_loss: 0.7580 - val_acc: 0.7051\n",
      "Epoch 187/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7487 - acc: 0.7048 - val_loss: 0.7570 - val_acc: 0.7043\n",
      "Epoch 188/1000\n",
      "5275/5275 [==============================] - 0s 73us/sample - loss: 0.7480 - acc: 0.7064 - val_loss: 0.7544 - val_acc: 0.7005\n",
      "Epoch 189/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7481 - acc: 0.7029 - val_loss: 0.7608 - val_acc: 0.6952\n",
      "Epoch 190/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7469 - acc: 0.7050 - val_loss: 0.7575 - val_acc: 0.7058\n",
      "Epoch 191/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7470 - acc: 0.7031 - val_loss: 0.7526 - val_acc: 0.7028\n",
      "Epoch 192/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7462 - acc: 0.7088 - val_loss: 0.7550 - val_acc: 0.7020\n",
      "Epoch 193/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7461 - acc: 0.7027 - val_loss: 0.7587 - val_acc: 0.6929\n",
      "Epoch 194/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7457 - acc: 0.7060 - val_loss: 0.7550 - val_acc: 0.7020\n",
      "Epoch 195/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7448 - acc: 0.7027 - val_loss: 0.7540 - val_acc: 0.7005\n",
      "Epoch 196/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7451 - acc: 0.7026 - val_loss: 0.7540 - val_acc: 0.7074\n",
      "Epoch 197/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7443 - acc: 0.7046 - val_loss: 0.7525 - val_acc: 0.7058\n",
      "Epoch 198/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7444 - acc: 0.7043 - val_loss: 0.7502 - val_acc: 0.7066\n",
      "Epoch 199/1000\n",
      "5275/5275 [==============================] - 0s 73us/sample - loss: 0.7426 - acc: 0.7052 - val_loss: 0.7490 - val_acc: 0.7005\n",
      "Epoch 200/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7429 - acc: 0.7050 - val_loss: 0.7505 - val_acc: 0.7013\n",
      "Epoch 201/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7423 - acc: 0.7056 - val_loss: 0.7504 - val_acc: 0.7005\n",
      "Epoch 202/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7419 - acc: 0.7045 - val_loss: 0.7478 - val_acc: 0.7020\n",
      "Epoch 203/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7416 - acc: 0.7058 - val_loss: 0.7520 - val_acc: 0.7058\n",
      "Epoch 204/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7410 - acc: 0.7052 - val_loss: 0.7469 - val_acc: 0.7020\n",
      "Epoch 205/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.7405 - acc: 0.7071 - val_loss: 0.7481 - val_acc: 0.7058\n",
      "Epoch 206/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7399 - acc: 0.7073 - val_loss: 0.7478 - val_acc: 0.7020\n",
      "Epoch 207/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7397 - acc: 0.7058 - val_loss: 0.7493 - val_acc: 0.7043\n",
      "Epoch 208/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7399 - acc: 0.7054 - val_loss: 0.7472 - val_acc: 0.6998\n",
      "Epoch 209/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.7391 - acc: 0.7037 - val_loss: 0.7468 - val_acc: 0.7043\n",
      "Epoch 210/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7385 - acc: 0.7037 - val_loss: 0.7487 - val_acc: 0.7020\n",
      "Epoch 211/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7385 - acc: 0.7035 - val_loss: 0.7474 - val_acc: 0.7013\n",
      "Epoch 212/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7385 - acc: 0.7050 - val_loss: 0.7455 - val_acc: 0.6998\n",
      "Epoch 213/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7377 - acc: 0.7067 - val_loss: 0.7492 - val_acc: 0.6967\n",
      "Epoch 214/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7374 - acc: 0.7041 - val_loss: 0.7447 - val_acc: 0.7051\n",
      "Epoch 215/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7372 - acc: 0.7050 - val_loss: 0.7486 - val_acc: 0.6952\n",
      "Epoch 216/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7368 - acc: 0.7039 - val_loss: 0.7427 - val_acc: 0.6967\n",
      "Epoch 217/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7367 - acc: 0.7065 - val_loss: 0.7477 - val_acc: 0.7020\n",
      "Epoch 218/1000\n",
      "5275/5275 [==============================] - 0s 84us/sample - loss: 0.7359 - acc: 0.7037 - val_loss: 0.7462 - val_acc: 0.6998\n",
      "Epoch 219/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7358 - acc: 0.7056 - val_loss: 0.7440 - val_acc: 0.7051\n",
      "Epoch 220/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7357 - acc: 0.7029 - val_loss: 0.7432 - val_acc: 0.7036\n",
      "Epoch 221/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7350 - acc: 0.7052 - val_loss: 0.7405 - val_acc: 0.6975\n",
      "Epoch 222/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.7346 - acc: 0.7046 - val_loss: 0.7443 - val_acc: 0.6990\n",
      "Epoch 223/1000\n",
      "5275/5275 [==============================] - 0s 83us/sample - loss: 0.7348 - acc: 0.7027 - val_loss: 0.7399 - val_acc: 0.6960\n",
      "Epoch 224/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7345 - acc: 0.7027 - val_loss: 0.7442 - val_acc: 0.6998\n",
      "Epoch 225/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7342 - acc: 0.7075 - val_loss: 0.7453 - val_acc: 0.7058\n",
      "Epoch 226/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7335 - acc: 0.7048 - val_loss: 0.7454 - val_acc: 0.7028\n",
      "Epoch 227/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7334 - acc: 0.7037 - val_loss: 0.7408 - val_acc: 0.7005\n",
      "Epoch 228/1000\n",
      "5275/5275 [==============================] - 0s 77us/sample - loss: 0.7332 - acc: 0.7056 - val_loss: 0.7397 - val_acc: 0.6990\n",
      "Epoch 229/1000\n",
      "5275/5275 [==============================] - 0s 79us/sample - loss: 0.7330 - acc: 0.7031 - val_loss: 0.7395 - val_acc: 0.7020\n",
      "Epoch 230/1000\n",
      "5275/5275 [==============================] - 0s 80us/sample - loss: 0.7324 - acc: 0.7064 - val_loss: 0.7400 - val_acc: 0.7020\n",
      "Epoch 231/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7325 - acc: 0.7037 - val_loss: 0.7384 - val_acc: 0.7020\n",
      "Epoch 232/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7319 - acc: 0.7052 - val_loss: 0.7421 - val_acc: 0.6960\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7315 - acc: 0.7022 - val_loss: 0.7420 - val_acc: 0.7013\n",
      "Epoch 234/1000\n",
      "5275/5275 [==============================] - 0s 81us/sample - loss: 0.7315 - acc: 0.7039 - val_loss: 0.7417 - val_acc: 0.6983\n",
      "Epoch 235/1000\n",
      "5275/5275 [==============================] - 0s 78us/sample - loss: 0.7315 - acc: 0.7069 - val_loss: 0.7418 - val_acc: 0.7074\n",
      "Epoch 236/1000\n",
      "5275/5275 [==============================] - 0s 82us/sample - loss: 0.7307 - acc: 0.7064 - val_loss: 0.7360 - val_acc: 0.6983\n",
      "Epoch 237/1000\n",
      "5275/5275 [==============================] - 0s 80us/sample - loss: 0.7303 - acc: 0.7077 - val_loss: 0.7361 - val_acc: 0.7020\n",
      "Epoch 238/1000\n",
      "5275/5275 [==============================] - 0s 79us/sample - loss: 0.7304 - acc: 0.7033 - val_loss: 0.7378 - val_acc: 0.6998\n",
      "Epoch 239/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 0.7302 - acc: 0.7065 - val_loss: 0.7390 - val_acc: 0.6983\n",
      "Epoch 240/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7298 - acc: 0.7031 - val_loss: 0.7395 - val_acc: 0.7020\n",
      "Epoch 241/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 0.7297 - acc: 0.7043 - val_loss: 0.7381 - val_acc: 0.7028\n",
      "Epoch 242/1000\n",
      "5275/5275 [==============================] - 0s 80us/sample - loss: 0.7293 - acc: 0.7037 - val_loss: 0.7345 - val_acc: 0.6952\n",
      "Epoch 243/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 0.7292 - acc: 0.7027 - val_loss: 0.7402 - val_acc: 0.6960\n",
      "Epoch 244/1000\n",
      "5275/5275 [==============================] - 0s 79us/sample - loss: 0.7287 - acc: 0.7071 - val_loss: 0.7339 - val_acc: 0.7020\n",
      "Epoch 245/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7285 - acc: 0.7073 - val_loss: 0.7388 - val_acc: 0.6975\n",
      "Epoch 246/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 0.7288 - acc: 0.7029 - val_loss: 0.7352 - val_acc: 0.6983\n",
      "Epoch 247/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 0.7279 - acc: 0.7039 - val_loss: 0.7409 - val_acc: 0.6998\n",
      "Epoch 248/1000\n",
      "5275/5275 [==============================] - 0s 80us/sample - loss: 0.7288 - acc: 0.7012 - val_loss: 0.7365 - val_acc: 0.7005\n",
      "Epoch 249/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7276 - acc: 0.7054 - val_loss: 0.7360 - val_acc: 0.6967\n",
      "Epoch 250/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7277 - acc: 0.7065 - val_loss: 0.7332 - val_acc: 0.6983\n",
      "Epoch 251/1000\n",
      "5275/5275 [==============================] - 0s 76us/sample - loss: 0.7272 - acc: 0.7065 - val_loss: 0.7380 - val_acc: 0.6990\n",
      "Epoch 252/1000\n",
      "5275/5275 [==============================] - 0s 78us/sample - loss: 0.7264 - acc: 0.7043 - val_loss: 0.7368 - val_acc: 0.6983\n",
      "Epoch 253/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 0.7270 - acc: 0.7062 - val_loss: 0.7324 - val_acc: 0.6998\n",
      "Epoch 254/1000\n",
      "5275/5275 [==============================] - 0s 79us/sample - loss: 0.7266 - acc: 0.7039 - val_loss: 0.7390 - val_acc: 0.7013\n",
      "Epoch 255/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7266 - acc: 0.7077 - val_loss: 0.7356 - val_acc: 0.6952\n",
      "Epoch 256/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 0.7259 - acc: 0.7046 - val_loss: 0.7349 - val_acc: 0.6967\n",
      "Epoch 257/1000\n",
      "5275/5275 [==============================] - 0s 83us/sample - loss: 0.7260 - acc: 0.7022 - val_loss: 0.7335 - val_acc: 0.6983\n",
      "Epoch 258/1000\n",
      "5275/5275 [==============================] - 0s 78us/sample - loss: 0.7254 - acc: 0.7062 - val_loss: 0.7322 - val_acc: 0.7013\n",
      "Epoch 259/1000\n",
      "5275/5275 [==============================] - 0s 76us/sample - loss: 0.7250 - acc: 0.7056 - val_loss: 0.7306 - val_acc: 0.6983\n",
      "Epoch 260/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7255 - acc: 0.7054 - val_loss: 0.7343 - val_acc: 0.7013\n",
      "Epoch 261/1000\n",
      "5275/5275 [==============================] - 0s 76us/sample - loss: 0.7247 - acc: 0.7048 - val_loss: 0.7332 - val_acc: 0.6967\n",
      "Epoch 262/1000\n",
      "5275/5275 [==============================] - 0s 77us/sample - loss: 0.7245 - acc: 0.7026 - val_loss: 0.7319 - val_acc: 0.7066\n",
      "Epoch 263/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7252 - acc: 0.7079 - val_loss: 0.7324 - val_acc: 0.6990\n",
      "Epoch 264/1000\n",
      "1920/5275 [=========>....................] - ETA: 0s - loss: 0.7202 - acc: 0.7172"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9dec8cdb3d75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 batch_size=32)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_epochs = int(1000)\n",
    "model.fit(new_data, new_label,\n",
    "                validation_split=0.2,\n",
    "                epochs=total_epochs,\n",
    "                batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 -> 2\n",
      "0.5 -> 3\n",
      "0.6 -> 6\n",
      "0.7 -> 7\n"
     ]
    }
   ],
   "source": [
    "for test in [0.4, 0.5, 0.6, 0.7]:\n",
    "    var = model.predict([test])\n",
    "    print(str(test) +\" -> \" + str(np.argmax(var)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sci\n",
    "from scipy.stats import chi2\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel, RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpdata = new_data.reshape(-1,1)\n",
    "gplabel = np.array(new_label)\n",
    "gplabel = gplabel.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-f023525c8948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConstantKernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mRBF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mWhiteKernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_level\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgpBad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgaussian_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianProcessRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgpBad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgplabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/sascha/.local/lib/python3.6/site-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    232\u001b[0m             optima = [(self._constrained_optimization(obj_func,\n\u001b[1;32m    233\u001b[0m                                                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                                                       self.kernel_.bounds))]\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# Additional runs are performed from log-uniform chosen initial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sascha/.local/lib/python3.6/site-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36m_constrained_optimization\u001b[0;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[1;32m    501\u001b[0m             opt_res = scipy.optimize.minimize(\n\u001b[1;32m    502\u001b[0m                 \u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L-BFGS-B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m                 bounds=bounds)\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0m_check_optimize_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mtheta_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sascha/.local/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m/home/sascha/.local/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sascha/.local/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sascha/.local/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sascha/.local/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sascha/.local/lib/python3.6/site-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(theta, eval_gradient)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                     lml, grad = self.log_marginal_likelihood(\n\u001b[0;32m--> 225\u001b[0;31m                         theta, eval_gradient=True, clone_kernel=False)\n\u001b[0m\u001b[1;32m    226\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sascha/.local/lib/python3.6/site-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mlog_marginal_likelihood\u001b[0;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sascha/.local/lib/python3.6/site-packages/sklearn/gaussian_process/kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0mK1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK1_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mK2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK2_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mK1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK1_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK2_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sascha/.local/lib/python3.6/site-packages/sklearn/gaussian_process/kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m             \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_level\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_noise_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kernel = ConstantKernel(1.0) * RBF(length_scale=1.0) + WhiteKernel(noise_level= 0.5)\n",
    "gpBad = gaussian_process.GaussianProcessRegressor(kernel=kernel)\n",
    "gpBad.fit(gpdata, gplabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linEvalTempBad = np.linspace(evalTempBad.min(), evalTempBad.max(), num=100).reshape(-1, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
