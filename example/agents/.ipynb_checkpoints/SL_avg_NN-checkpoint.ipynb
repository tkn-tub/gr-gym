{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('agentdata/rssi_sl_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset of best action for each observation (obs and label)\n",
    "cols = []\n",
    "for i in range(64):\n",
    "    cols.append('Sc' + str(i))\n",
    "\n",
    "df['Run'] = df.apply(lambda row: int(row.name / 8), axis=1)\n",
    "df = df[df['Reward'] > 0.0]\n",
    "\n",
    "obs = []\n",
    "label = []\n",
    "\n",
    "for run in df['Run'].unique():\n",
    "    runData = df[df['Run'] == run]\n",
    "    myid = runData['Reward'].idxmax()\n",
    "    myrow = df.loc[myid]\n",
    "    label.append(myrow['Action'])\n",
    "    obs.append(myrow[cols].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Dist'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 7.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_size = 8\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(1, input_shape=(1,), activation='sigmoid'))\n",
    "model.add(keras.layers.Dense(a_size, activation='relu'))\n",
    "#model.add(keras.layers.Dense(a_size, activation='relu'))\n",
    "model.add(keras.layers.Dense(a_size, activation='softmax'))\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take average observation over all 64 subcarriers\n",
    "data = []\n",
    "for elem in obs:\n",
    "    elem = np.average(elem)\n",
    "    elem = (elem - 0) / 45 \n",
    "    elem = np.reshape(elem, [1, ])\n",
    "    data.append(elem)\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Action')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data, label, '.', alpha = 0.1)\n",
    "plt.xlabel('Observation (Average)')\n",
    "plt.ylabel('Action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomize data\n",
    "new_data = data\n",
    "new_label =[]\n",
    "new_label.extend(label)\n",
    "\n",
    "for i in range(5):\n",
    "    new_data = np.append(new_data, data + np.random.normal(0,0.005,len(data)).reshape(len(data),1))\n",
    "    new_label.extend(label)\n",
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5275 samples, validate on 1319 samples\n",
      "Epoch 1/1000\n",
      "5275/5275 [==============================] - 1s 124us/sample - loss: 1.9834 - acc: 0.2292 - val_loss: 1.9480 - val_acc: 0.2578\n",
      "Epoch 2/1000\n",
      "5275/5275 [==============================] - 0s 73us/sample - loss: 1.9311 - acc: 0.2620 - val_loss: 1.9230 - val_acc: 0.2578\n",
      "Epoch 3/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 1.9154 - acc: 0.2620 - val_loss: 1.9128 - val_acc: 0.2578\n",
      "Epoch 4/1000\n",
      "5275/5275 [==============================] - 0s 77us/sample - loss: 1.9081 - acc: 0.2620 - val_loss: 1.9073 - val_acc: 0.2578\n",
      "Epoch 5/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.9046 - acc: 0.2620 - val_loss: 1.9047 - val_acc: 0.2578\n",
      "Epoch 6/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 1.9025 - acc: 0.2620 - val_loss: 1.9032 - val_acc: 0.2578\n",
      "Epoch 7/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 1.9012 - acc: 0.2620 - val_loss: 1.9018 - val_acc: 0.2578\n",
      "Epoch 8/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.9002 - acc: 0.2620 - val_loss: 1.9003 - val_acc: 0.2578\n",
      "Epoch 9/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 1.8991 - acc: 0.2620 - val_loss: 1.8998 - val_acc: 0.2578\n",
      "Epoch 10/1000\n",
      "5275/5275 [==============================] - 0s 76us/sample - loss: 1.8972 - acc: 0.2620 - val_loss: 1.8987 - val_acc: 0.2578\n",
      "Epoch 11/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 1.8965 - acc: 0.2620 - val_loss: 1.8969 - val_acc: 0.2578\n",
      "Epoch 12/1000\n",
      "5275/5275 [==============================] - 0s 80us/sample - loss: 1.8950 - acc: 0.2620 - val_loss: 1.8953 - val_acc: 0.2578\n",
      "Epoch 13/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.8932 - acc: 0.2620 - val_loss: 1.8934 - val_acc: 0.2578\n",
      "Epoch 14/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 1.8911 - acc: 0.2620 - val_loss: 1.8912 - val_acc: 0.2578\n",
      "Epoch 15/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 1.8891 - acc: 0.2620 - val_loss: 1.8887 - val_acc: 0.2578\n",
      "Epoch 16/1000\n",
      "5275/5275 [==============================] - 0s 76us/sample - loss: 1.8864 - acc: 0.2620 - val_loss: 1.8859 - val_acc: 0.2578\n",
      "Epoch 17/1000\n",
      "5275/5275 [==============================] - 0s 78us/sample - loss: 1.8832 - acc: 0.2620 - val_loss: 1.8822 - val_acc: 0.2578\n",
      "Epoch 18/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 1.8790 - acc: 0.2620 - val_loss: 1.8783 - val_acc: 0.2578\n",
      "Epoch 19/1000\n",
      "5275/5275 [==============================] - 0s 87us/sample - loss: 1.8741 - acc: 0.2620 - val_loss: 1.8723 - val_acc: 0.2578\n",
      "Epoch 20/1000\n",
      "5275/5275 [==============================] - 0s 77us/sample - loss: 1.8678 - acc: 0.2620 - val_loss: 1.8656 - val_acc: 0.2578\n",
      "Epoch 21/1000\n",
      "5275/5275 [==============================] - 0s 88us/sample - loss: 1.8605 - acc: 0.2620 - val_loss: 1.8569 - val_acc: 0.2578\n",
      "Epoch 22/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 1.8505 - acc: 0.2620 - val_loss: 1.8460 - val_acc: 0.2578\n",
      "Epoch 23/1000\n",
      "5275/5275 [==============================] - 0s 78us/sample - loss: 1.8383 - acc: 0.2620 - val_loss: 1.8322 - val_acc: 0.2578\n",
      "Epoch 24/1000\n",
      "5275/5275 [==============================] - 0s 78us/sample - loss: 1.8231 - acc: 0.2620 - val_loss: 1.8149 - val_acc: 0.2578\n",
      "Epoch 25/1000\n",
      "5275/5275 [==============================] - 0s 83us/sample - loss: 1.8042 - acc: 0.2627 - val_loss: 1.7937 - val_acc: 0.2585\n",
      "Epoch 26/1000\n",
      "5275/5275 [==============================] - 0s 73us/sample - loss: 1.7812 - acc: 0.2618 - val_loss: 1.7698 - val_acc: 0.2593\n",
      "Epoch 27/1000\n",
      "5275/5275 [==============================] - 0s 86us/sample - loss: 1.7551 - acc: 0.2639 - val_loss: 1.7420 - val_acc: 0.2593\n",
      "Epoch 28/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 1.7257 - acc: 0.2690 - val_loss: 1.7110 - val_acc: 0.2896\n",
      "Epoch 29/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 1.6930 - acc: 0.3090 - val_loss: 1.6771 - val_acc: 0.3313\n",
      "Epoch 30/1000\n",
      "5275/5275 [==============================] - 0s 86us/sample - loss: 1.6585 - acc: 0.3456 - val_loss: 1.6415 - val_acc: 0.3692\n",
      "Epoch 31/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 1.6227 - acc: 0.3852 - val_loss: 1.6056 - val_acc: 0.3867\n",
      "Epoch 32/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 1.5860 - acc: 0.3981 - val_loss: 1.5689 - val_acc: 0.3920\n",
      "Epoch 33/1000\n",
      "5275/5275 [==============================] - 0s 76us/sample - loss: 1.5496 - acc: 0.4044 - val_loss: 1.5329 - val_acc: 0.3980\n",
      "Epoch 34/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 1.5144 - acc: 0.4076 - val_loss: 1.4990 - val_acc: 0.3995\n",
      "Epoch 35/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.4814 - acc: 0.4089 - val_loss: 1.4687 - val_acc: 0.4011\n",
      "Epoch 36/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 1.4513 - acc: 0.4227 - val_loss: 1.4391 - val_acc: 0.4230\n",
      "Epoch 37/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 1.4239 - acc: 0.4402 - val_loss: 1.4131 - val_acc: 0.4329\n",
      "Epoch 38/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.3997 - acc: 0.4504 - val_loss: 1.3896 - val_acc: 0.4412\n",
      "Epoch 39/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.3774 - acc: 0.4584 - val_loss: 1.3689 - val_acc: 0.4511\n",
      "Epoch 40/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.3569 - acc: 0.4664 - val_loss: 1.3492 - val_acc: 0.4579\n",
      "Epoch 41/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.3378 - acc: 0.4724 - val_loss: 1.3307 - val_acc: 0.4594\n",
      "Epoch 42/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.3198 - acc: 0.4756 - val_loss: 1.3127 - val_acc: 0.4655\n",
      "Epoch 43/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.3029 - acc: 0.4811 - val_loss: 1.2963 - val_acc: 0.4701\n",
      "Epoch 44/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.2869 - acc: 0.4929 - val_loss: 1.2814 - val_acc: 0.4754\n",
      "Epoch 45/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 1.2717 - acc: 0.4967 - val_loss: 1.2651 - val_acc: 0.4875\n",
      "Epoch 46/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 1.2571 - acc: 0.4978 - val_loss: 1.2514 - val_acc: 0.5110\n",
      "Epoch 47/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.2435 - acc: 0.5225 - val_loss: 1.2381 - val_acc: 0.5042\n",
      "Epoch 48/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.2300 - acc: 0.5196 - val_loss: 1.2249 - val_acc: 0.5337\n",
      "Epoch 49/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.2170 - acc: 0.5312 - val_loss: 1.2122 - val_acc: 0.5178\n",
      "Epoch 50/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.2048 - acc: 0.5371 - val_loss: 1.1996 - val_acc: 0.5633\n",
      "Epoch 51/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.1924 - acc: 0.5653 - val_loss: 1.1883 - val_acc: 0.5155\n",
      "Epoch 52/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 1.1810 - acc: 0.5492 - val_loss: 1.1769 - val_acc: 0.5421\n",
      "Epoch 53/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.1697 - acc: 0.5522 - val_loss: 1.1668 - val_acc: 0.5398\n",
      "Epoch 54/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.1585 - acc: 0.5647 - val_loss: 1.1543 - val_acc: 0.5777\n",
      "Epoch 55/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.1480 - acc: 0.5750 - val_loss: 1.1445 - val_acc: 0.5770\n",
      "Epoch 56/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.1375 - acc: 0.5846 - val_loss: 1.1345 - val_acc: 0.5519\n",
      "Epoch 57/1000\n",
      "5275/5275 [==============================] - 0s 64us/sample - loss: 1.1270 - acc: 0.5742 - val_loss: 1.1237 - val_acc: 0.5861\n",
      "Epoch 58/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.1170 - acc: 0.5882 - val_loss: 1.1138 - val_acc: 0.5906\n",
      "Epoch 59/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.1073 - acc: 0.5956 - val_loss: 1.1065 - val_acc: 0.5557\n",
      "Epoch 60/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 1.0975 - acc: 0.5879 - val_loss: 1.0938 - val_acc: 0.5997\n",
      "Epoch 61/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 1.0881 - acc: 0.5989 - val_loss: 1.0850 - val_acc: 0.5944\n",
      "Epoch 62/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 1.0789 - acc: 0.6051 - val_loss: 1.0757 - val_acc: 0.5997\n",
      "Epoch 63/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 1.0698 - acc: 0.6013 - val_loss: 1.0669 - val_acc: 0.6133\n",
      "Epoch 64/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.0611 - acc: 0.6142 - val_loss: 1.0608 - val_acc: 0.5732\n",
      "Epoch 65/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 1.0529 - acc: 0.6097 - val_loss: 1.0516 - val_acc: 0.6020\n",
      "Epoch 66/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 1.0446 - acc: 0.6135 - val_loss: 1.0430 - val_acc: 0.6088\n",
      "Epoch 67/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.0364 - acc: 0.6150 - val_loss: 1.0355 - val_acc: 0.6171\n",
      "Epoch 68/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 1.0291 - acc: 0.6239 - val_loss: 1.0266 - val_acc: 0.6187\n",
      "Epoch 69/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 1.0211 - acc: 0.6222 - val_loss: 1.0215 - val_acc: 0.6187\n",
      "Epoch 70/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 1.0139 - acc: 0.6245 - val_loss: 1.0119 - val_acc: 0.6300\n",
      "Epoch 71/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 1.0070 - acc: 0.6298 - val_loss: 1.0078 - val_acc: 0.6141\n",
      "Epoch 72/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 1.0006 - acc: 0.6315 - val_loss: 1.0015 - val_acc: 0.6103\n",
      "Epoch 73/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.9936 - acc: 0.6290 - val_loss: 0.9948 - val_acc: 0.6277\n",
      "Epoch 74/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.9872 - acc: 0.6332 - val_loss: 0.9887 - val_acc: 0.6194\n",
      "Epoch 75/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 0.9810 - acc: 0.6337 - val_loss: 0.9828 - val_acc: 0.6164\n",
      "Epoch 76/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.9749 - acc: 0.6311 - val_loss: 0.9736 - val_acc: 0.6376\n",
      "Epoch 77/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.9687 - acc: 0.6360 - val_loss: 0.9688 - val_acc: 0.6315\n",
      "Epoch 78/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.9629 - acc: 0.6377 - val_loss: 0.9637 - val_acc: 0.6270\n",
      "Epoch 79/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.9578 - acc: 0.6337 - val_loss: 0.9574 - val_acc: 0.6353\n",
      "Epoch 80/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.9519 - acc: 0.6398 - val_loss: 0.9514 - val_acc: 0.6429\n",
      "Epoch 81/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.9465 - acc: 0.6349 - val_loss: 0.9490 - val_acc: 0.6315\n",
      "Epoch 82/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.9412 - acc: 0.6383 - val_loss: 0.9411 - val_acc: 0.6444\n",
      "Epoch 83/1000\n",
      "5275/5275 [==============================] - 0s 73us/sample - loss: 0.9363 - acc: 0.6398 - val_loss: 0.9363 - val_acc: 0.6406\n",
      "Epoch 84/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.9316 - acc: 0.6425 - val_loss: 0.9322 - val_acc: 0.6391\n",
      "Epoch 85/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.9267 - acc: 0.6398 - val_loss: 0.9267 - val_acc: 0.6444\n",
      "Epoch 86/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.9221 - acc: 0.6451 - val_loss: 0.9243 - val_acc: 0.6361\n",
      "Epoch 87/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.9172 - acc: 0.6447 - val_loss: 0.9177 - val_acc: 0.6755\n",
      "Epoch 88/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.9130 - acc: 0.6601 - val_loss: 0.9142 - val_acc: 0.6657\n",
      "Epoch 89/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.9091 - acc: 0.6701 - val_loss: 0.9105 - val_acc: 0.6717\n",
      "Epoch 90/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.9046 - acc: 0.6745 - val_loss: 0.9055 - val_acc: 0.6846\n",
      "Epoch 91/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.9009 - acc: 0.6777 - val_loss: 0.9024 - val_acc: 0.6793\n",
      "Epoch 92/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.8966 - acc: 0.6773 - val_loss: 0.8983 - val_acc: 0.6854\n",
      "Epoch 93/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8930 - acc: 0.6808 - val_loss: 0.8953 - val_acc: 0.6801\n",
      "Epoch 94/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.8889 - acc: 0.6789 - val_loss: 0.8910 - val_acc: 0.6839\n",
      "Epoch 95/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8849 - acc: 0.6819 - val_loss: 0.8888 - val_acc: 0.6785\n",
      "Epoch 96/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.8816 - acc: 0.6828 - val_loss: 0.8835 - val_acc: 0.6876\n",
      "Epoch 97/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8782 - acc: 0.6859 - val_loss: 0.8832 - val_acc: 0.6816\n",
      "Epoch 98/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.8745 - acc: 0.6825 - val_loss: 0.8766 - val_acc: 0.6892\n",
      "Epoch 99/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.8714 - acc: 0.6845 - val_loss: 0.8736 - val_acc: 0.6869\n",
      "Epoch 100/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.8682 - acc: 0.6840 - val_loss: 0.8711 - val_acc: 0.6876\n",
      "Epoch 101/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8650 - acc: 0.6859 - val_loss: 0.8705 - val_acc: 0.6785\n",
      "Epoch 102/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8616 - acc: 0.6863 - val_loss: 0.8671 - val_acc: 0.6808\n",
      "Epoch 103/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8590 - acc: 0.6851 - val_loss: 0.8634 - val_acc: 0.6899\n",
      "Epoch 104/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.8558 - acc: 0.6874 - val_loss: 0.8587 - val_acc: 0.6937\n",
      "Epoch 105/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.8530 - acc: 0.6900 - val_loss: 0.8600 - val_acc: 0.6801\n",
      "Epoch 106/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.8503 - acc: 0.6906 - val_loss: 0.8529 - val_acc: 0.6884\n",
      "Epoch 107/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8473 - acc: 0.6902 - val_loss: 0.8514 - val_acc: 0.6952\n",
      "Epoch 108/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.8447 - acc: 0.6912 - val_loss: 0.8506 - val_acc: 0.6839\n",
      "Epoch 109/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8422 - acc: 0.6910 - val_loss: 0.8474 - val_acc: 0.6884\n",
      "Epoch 110/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8397 - acc: 0.6904 - val_loss: 0.8431 - val_acc: 0.6892\n",
      "Epoch 111/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.8371 - acc: 0.6887 - val_loss: 0.8407 - val_acc: 0.6892\n",
      "Epoch 112/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.8345 - acc: 0.6912 - val_loss: 0.8418 - val_acc: 0.6823\n",
      "Epoch 113/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.8326 - acc: 0.6897 - val_loss: 0.8379 - val_acc: 0.6998\n",
      "Epoch 114/1000\n",
      "5275/5275 [==============================] - 0s 64us/sample - loss: 0.8303 - acc: 0.6954 - val_loss: 0.8358 - val_acc: 0.6937\n",
      "Epoch 115/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.8279 - acc: 0.6925 - val_loss: 0.8366 - val_acc: 0.6899\n",
      "Epoch 116/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8264 - acc: 0.6938 - val_loss: 0.8350 - val_acc: 0.6861\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.8239 - acc: 0.6955 - val_loss: 0.8274 - val_acc: 0.7051\n",
      "Epoch 118/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.8214 - acc: 0.6959 - val_loss: 0.8268 - val_acc: 0.6899\n",
      "Epoch 119/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.8200 - acc: 0.6942 - val_loss: 0.8239 - val_acc: 0.6983\n",
      "Epoch 120/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.8177 - acc: 0.6973 - val_loss: 0.8226 - val_acc: 0.6922\n",
      "Epoch 121/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.8159 - acc: 0.6976 - val_loss: 0.8205 - val_acc: 0.7005\n",
      "Epoch 122/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8138 - acc: 0.6963 - val_loss: 0.8192 - val_acc: 0.6952\n",
      "Epoch 123/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.8123 - acc: 0.6936 - val_loss: 0.8160 - val_acc: 0.6998\n",
      "Epoch 124/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.8104 - acc: 0.6967 - val_loss: 0.8150 - val_acc: 0.7020\n",
      "Epoch 125/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.8082 - acc: 0.6999 - val_loss: 0.8122 - val_acc: 0.6960\n",
      "Epoch 126/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.8071 - acc: 0.6952 - val_loss: 0.8127 - val_acc: 0.7081\n",
      "Epoch 127/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.8054 - acc: 0.6980 - val_loss: 0.8133 - val_acc: 0.6975\n",
      "Epoch 128/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.8040 - acc: 0.6963 - val_loss: 0.8112 - val_acc: 0.7036\n",
      "Epoch 129/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.8017 - acc: 0.7001 - val_loss: 0.8064 - val_acc: 0.6975\n",
      "Epoch 130/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.8003 - acc: 0.6984 - val_loss: 0.8097 - val_acc: 0.6975\n",
      "Epoch 131/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7990 - acc: 0.6995 - val_loss: 0.8066 - val_acc: 0.6937\n",
      "Epoch 132/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7977 - acc: 0.7007 - val_loss: 0.8084 - val_acc: 0.6952\n",
      "Epoch 133/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7967 - acc: 0.6978 - val_loss: 0.8027 - val_acc: 0.7013\n",
      "Epoch 134/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7944 - acc: 0.7001 - val_loss: 0.7993 - val_acc: 0.6990\n",
      "Epoch 135/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7933 - acc: 0.6976 - val_loss: 0.7974 - val_acc: 0.7074\n",
      "Epoch 136/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7919 - acc: 0.6988 - val_loss: 0.7969 - val_acc: 0.7058\n",
      "Epoch 137/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7906 - acc: 0.6988 - val_loss: 0.7954 - val_acc: 0.6983\n",
      "Epoch 138/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7893 - acc: 0.7014 - val_loss: 0.7957 - val_acc: 0.7058\n",
      "Epoch 139/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7875 - acc: 0.7007 - val_loss: 0.7925 - val_acc: 0.6929\n",
      "Epoch 140/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7867 - acc: 0.6990 - val_loss: 0.7924 - val_acc: 0.7013\n",
      "Epoch 141/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7857 - acc: 0.6993 - val_loss: 0.7950 - val_acc: 0.6937\n",
      "Epoch 142/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7851 - acc: 0.7012 - val_loss: 0.7914 - val_acc: 0.7066\n",
      "Epoch 143/1000\n",
      "5275/5275 [==============================] - 0s 73us/sample - loss: 0.7833 - acc: 0.7037 - val_loss: 0.7891 - val_acc: 0.7089\n",
      "Epoch 144/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7819 - acc: 0.7020 - val_loss: 0.7885 - val_acc: 0.7020\n",
      "Epoch 145/1000\n",
      "5275/5275 [==============================] - 0s 82us/sample - loss: 0.7809 - acc: 0.7018 - val_loss: 0.7865 - val_acc: 0.7005\n",
      "Epoch 146/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7798 - acc: 0.7007 - val_loss: 0.7863 - val_acc: 0.7058\n",
      "Epoch 147/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7788 - acc: 0.7007 - val_loss: 0.7831 - val_acc: 0.7043\n",
      "Epoch 148/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7776 - acc: 0.6986 - val_loss: 0.7819 - val_acc: 0.6998\n",
      "Epoch 149/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7769 - acc: 0.7041 - val_loss: 0.7828 - val_acc: 0.7058\n",
      "Epoch 150/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.7760 - acc: 0.7003 - val_loss: 0.7825 - val_acc: 0.7036\n",
      "Epoch 151/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7744 - acc: 0.7010 - val_loss: 0.7810 - val_acc: 0.7043\n",
      "Epoch 152/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7737 - acc: 0.7029 - val_loss: 0.7815 - val_acc: 0.7081\n",
      "Epoch 153/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7724 - acc: 0.7056 - val_loss: 0.7805 - val_acc: 0.7089\n",
      "Epoch 154/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7717 - acc: 0.7031 - val_loss: 0.7797 - val_acc: 0.7028\n",
      "Epoch 155/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7709 - acc: 0.6993 - val_loss: 0.7763 - val_acc: 0.7043\n",
      "Epoch 156/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7699 - acc: 0.7035 - val_loss: 0.7789 - val_acc: 0.6983\n",
      "Epoch 157/1000\n",
      "5275/5275 [==============================] - 0s 64us/sample - loss: 0.7694 - acc: 0.6988 - val_loss: 0.7747 - val_acc: 0.7036\n",
      "Epoch 158/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7680 - acc: 0.7027 - val_loss: 0.7742 - val_acc: 0.7036\n",
      "Epoch 159/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.7674 - acc: 0.7016 - val_loss: 0.7720 - val_acc: 0.7028\n",
      "Epoch 160/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7669 - acc: 0.7022 - val_loss: 0.7715 - val_acc: 0.7036\n",
      "Epoch 161/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7661 - acc: 0.7003 - val_loss: 0.7739 - val_acc: 0.7005\n",
      "Epoch 162/1000\n",
      "5275/5275 [==============================] - 0s 64us/sample - loss: 0.7650 - acc: 0.7039 - val_loss: 0.7740 - val_acc: 0.6952\n",
      "Epoch 163/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7643 - acc: 0.7022 - val_loss: 0.7707 - val_acc: 0.6998\n",
      "Epoch 164/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7634 - acc: 0.7035 - val_loss: 0.7691 - val_acc: 0.7013\n",
      "Epoch 165/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7624 - acc: 0.7043 - val_loss: 0.7694 - val_acc: 0.7043\n",
      "Epoch 166/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7616 - acc: 0.7014 - val_loss: 0.7682 - val_acc: 0.7028\n",
      "Epoch 167/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7612 - acc: 0.7033 - val_loss: 0.7676 - val_acc: 0.7066\n",
      "Epoch 168/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7604 - acc: 0.7020 - val_loss: 0.7652 - val_acc: 0.7013\n",
      "Epoch 169/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7599 - acc: 0.7054 - val_loss: 0.7687 - val_acc: 0.6945\n",
      "Epoch 170/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7586 - acc: 0.7041 - val_loss: 0.7641 - val_acc: 0.7036\n",
      "Epoch 171/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7586 - acc: 0.7054 - val_loss: 0.7678 - val_acc: 0.7028\n",
      "Epoch 172/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.7575 - acc: 0.7037 - val_loss: 0.7629 - val_acc: 0.7043\n",
      "Epoch 173/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7573 - acc: 0.7045 - val_loss: 0.7656 - val_acc: 0.7005\n",
      "Epoch 174/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7562 - acc: 0.7067 - val_loss: 0.7643 - val_acc: 0.7005\n",
      "Epoch 175/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7558 - acc: 0.7056 - val_loss: 0.7633 - val_acc: 0.7051\n",
      "Epoch 176/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7550 - acc: 0.7056 - val_loss: 0.7653 - val_acc: 0.6975\n",
      "Epoch 177/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7544 - acc: 0.7048 - val_loss: 0.7640 - val_acc: 0.7043\n",
      "Epoch 178/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7538 - acc: 0.7060 - val_loss: 0.7590 - val_acc: 0.7051\n",
      "Epoch 179/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7534 - acc: 0.7045 - val_loss: 0.7642 - val_acc: 0.7058\n",
      "Epoch 180/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7529 - acc: 0.7014 - val_loss: 0.7590 - val_acc: 0.7036\n",
      "Epoch 181/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7523 - acc: 0.7060 - val_loss: 0.7582 - val_acc: 0.7028\n",
      "Epoch 182/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7510 - acc: 0.7050 - val_loss: 0.7632 - val_acc: 0.7036\n",
      "Epoch 183/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7508 - acc: 0.7039 - val_loss: 0.7604 - val_acc: 0.7013\n",
      "Epoch 184/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7508 - acc: 0.7062 - val_loss: 0.7600 - val_acc: 0.6998\n",
      "Epoch 185/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7504 - acc: 0.7024 - val_loss: 0.7555 - val_acc: 0.7020\n",
      "Epoch 186/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7492 - acc: 0.7045 - val_loss: 0.7580 - val_acc: 0.7051\n",
      "Epoch 187/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7487 - acc: 0.7048 - val_loss: 0.7570 - val_acc: 0.7043\n",
      "Epoch 188/1000\n",
      "5275/5275 [==============================] - 0s 73us/sample - loss: 0.7480 - acc: 0.7064 - val_loss: 0.7544 - val_acc: 0.7005\n",
      "Epoch 189/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7481 - acc: 0.7029 - val_loss: 0.7608 - val_acc: 0.6952\n",
      "Epoch 190/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7469 - acc: 0.7050 - val_loss: 0.7575 - val_acc: 0.7058\n",
      "Epoch 191/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7470 - acc: 0.7031 - val_loss: 0.7526 - val_acc: 0.7028\n",
      "Epoch 192/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7462 - acc: 0.7088 - val_loss: 0.7550 - val_acc: 0.7020\n",
      "Epoch 193/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7461 - acc: 0.7027 - val_loss: 0.7587 - val_acc: 0.6929\n",
      "Epoch 194/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7457 - acc: 0.7060 - val_loss: 0.7550 - val_acc: 0.7020\n",
      "Epoch 195/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7448 - acc: 0.7027 - val_loss: 0.7540 - val_acc: 0.7005\n",
      "Epoch 196/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7451 - acc: 0.7026 - val_loss: 0.7540 - val_acc: 0.7074\n",
      "Epoch 197/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7443 - acc: 0.7046 - val_loss: 0.7525 - val_acc: 0.7058\n",
      "Epoch 198/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7444 - acc: 0.7043 - val_loss: 0.7502 - val_acc: 0.7066\n",
      "Epoch 199/1000\n",
      "5275/5275 [==============================] - 0s 73us/sample - loss: 0.7426 - acc: 0.7052 - val_loss: 0.7490 - val_acc: 0.7005\n",
      "Epoch 200/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7429 - acc: 0.7050 - val_loss: 0.7505 - val_acc: 0.7013\n",
      "Epoch 201/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7423 - acc: 0.7056 - val_loss: 0.7504 - val_acc: 0.7005\n",
      "Epoch 202/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7419 - acc: 0.7045 - val_loss: 0.7478 - val_acc: 0.7020\n",
      "Epoch 203/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7416 - acc: 0.7058 - val_loss: 0.7520 - val_acc: 0.7058\n",
      "Epoch 204/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7410 - acc: 0.7052 - val_loss: 0.7469 - val_acc: 0.7020\n",
      "Epoch 205/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.7405 - acc: 0.7071 - val_loss: 0.7481 - val_acc: 0.7058\n",
      "Epoch 206/1000\n",
      "5275/5275 [==============================] - 0s 66us/sample - loss: 0.7399 - acc: 0.7073 - val_loss: 0.7478 - val_acc: 0.7020\n",
      "Epoch 207/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7397 - acc: 0.7058 - val_loss: 0.7493 - val_acc: 0.7043\n",
      "Epoch 208/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7399 - acc: 0.7054 - val_loss: 0.7472 - val_acc: 0.6998\n",
      "Epoch 209/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.7391 - acc: 0.7037 - val_loss: 0.7468 - val_acc: 0.7043\n",
      "Epoch 210/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7385 - acc: 0.7037 - val_loss: 0.7487 - val_acc: 0.7020\n",
      "Epoch 211/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7385 - acc: 0.7035 - val_loss: 0.7474 - val_acc: 0.7013\n",
      "Epoch 212/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7385 - acc: 0.7050 - val_loss: 0.7455 - val_acc: 0.6998\n",
      "Epoch 213/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7377 - acc: 0.7067 - val_loss: 0.7492 - val_acc: 0.6967\n",
      "Epoch 214/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7374 - acc: 0.7041 - val_loss: 0.7447 - val_acc: 0.7051\n",
      "Epoch 215/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7372 - acc: 0.7050 - val_loss: 0.7486 - val_acc: 0.6952\n",
      "Epoch 216/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7368 - acc: 0.7039 - val_loss: 0.7427 - val_acc: 0.6967\n",
      "Epoch 217/1000\n",
      "5275/5275 [==============================] - 0s 68us/sample - loss: 0.7367 - acc: 0.7065 - val_loss: 0.7477 - val_acc: 0.7020\n",
      "Epoch 218/1000\n",
      "5275/5275 [==============================] - 0s 84us/sample - loss: 0.7359 - acc: 0.7037 - val_loss: 0.7462 - val_acc: 0.6998\n",
      "Epoch 219/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7358 - acc: 0.7056 - val_loss: 0.7440 - val_acc: 0.7051\n",
      "Epoch 220/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7357 - acc: 0.7029 - val_loss: 0.7432 - val_acc: 0.7036\n",
      "Epoch 221/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7350 - acc: 0.7052 - val_loss: 0.7405 - val_acc: 0.6975\n",
      "Epoch 222/1000\n",
      "5275/5275 [==============================] - 0s 71us/sample - loss: 0.7346 - acc: 0.7046 - val_loss: 0.7443 - val_acc: 0.6990\n",
      "Epoch 223/1000\n",
      "5275/5275 [==============================] - 0s 83us/sample - loss: 0.7348 - acc: 0.7027 - val_loss: 0.7399 - val_acc: 0.6960\n",
      "Epoch 224/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7345 - acc: 0.7027 - val_loss: 0.7442 - val_acc: 0.6998\n",
      "Epoch 225/1000\n",
      "5275/5275 [==============================] - 0s 67us/sample - loss: 0.7342 - acc: 0.7075 - val_loss: 0.7453 - val_acc: 0.7058\n",
      "Epoch 226/1000\n",
      "5275/5275 [==============================] - 0s 65us/sample - loss: 0.7335 - acc: 0.7048 - val_loss: 0.7454 - val_acc: 0.7028\n",
      "Epoch 227/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7334 - acc: 0.7037 - val_loss: 0.7408 - val_acc: 0.7005\n",
      "Epoch 228/1000\n",
      "5275/5275 [==============================] - 0s 77us/sample - loss: 0.7332 - acc: 0.7056 - val_loss: 0.7397 - val_acc: 0.6990\n",
      "Epoch 229/1000\n",
      "5275/5275 [==============================] - 0s 79us/sample - loss: 0.7330 - acc: 0.7031 - val_loss: 0.7395 - val_acc: 0.7020\n",
      "Epoch 230/1000\n",
      "5275/5275 [==============================] - 0s 80us/sample - loss: 0.7324 - acc: 0.7064 - val_loss: 0.7400 - val_acc: 0.7020\n",
      "Epoch 231/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7325 - acc: 0.7037 - val_loss: 0.7384 - val_acc: 0.7020\n",
      "Epoch 232/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7319 - acc: 0.7052 - val_loss: 0.7421 - val_acc: 0.6960\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7315 - acc: 0.7022 - val_loss: 0.7420 - val_acc: 0.7013\n",
      "Epoch 234/1000\n",
      "5275/5275 [==============================] - 0s 81us/sample - loss: 0.7315 - acc: 0.7039 - val_loss: 0.7417 - val_acc: 0.6983\n",
      "Epoch 235/1000\n",
      "5275/5275 [==============================] - 0s 78us/sample - loss: 0.7315 - acc: 0.7069 - val_loss: 0.7418 - val_acc: 0.7074\n",
      "Epoch 236/1000\n",
      "5275/5275 [==============================] - 0s 82us/sample - loss: 0.7307 - acc: 0.7064 - val_loss: 0.7360 - val_acc: 0.6983\n",
      "Epoch 237/1000\n",
      "5275/5275 [==============================] - 0s 80us/sample - loss: 0.7303 - acc: 0.7077 - val_loss: 0.7361 - val_acc: 0.7020\n",
      "Epoch 238/1000\n",
      "5275/5275 [==============================] - 0s 79us/sample - loss: 0.7304 - acc: 0.7033 - val_loss: 0.7378 - val_acc: 0.6998\n",
      "Epoch 239/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 0.7302 - acc: 0.7065 - val_loss: 0.7390 - val_acc: 0.6983\n",
      "Epoch 240/1000\n",
      "5275/5275 [==============================] - 0s 69us/sample - loss: 0.7298 - acc: 0.7031 - val_loss: 0.7395 - val_acc: 0.7020\n",
      "Epoch 241/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 0.7297 - acc: 0.7043 - val_loss: 0.7381 - val_acc: 0.7028\n",
      "Epoch 242/1000\n",
      "5275/5275 [==============================] - 0s 80us/sample - loss: 0.7293 - acc: 0.7037 - val_loss: 0.7345 - val_acc: 0.6952\n",
      "Epoch 243/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 0.7292 - acc: 0.7027 - val_loss: 0.7402 - val_acc: 0.6960\n",
      "Epoch 244/1000\n",
      "5275/5275 [==============================] - 0s 79us/sample - loss: 0.7287 - acc: 0.7071 - val_loss: 0.7339 - val_acc: 0.7020\n",
      "Epoch 245/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7285 - acc: 0.7073 - val_loss: 0.7388 - val_acc: 0.6975\n",
      "Epoch 246/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 0.7288 - acc: 0.7029 - val_loss: 0.7352 - val_acc: 0.6983\n",
      "Epoch 247/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 0.7279 - acc: 0.7039 - val_loss: 0.7409 - val_acc: 0.6998\n",
      "Epoch 248/1000\n",
      "5275/5275 [==============================] - 0s 80us/sample - loss: 0.7288 - acc: 0.7012 - val_loss: 0.7365 - val_acc: 0.7005\n",
      "Epoch 249/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7276 - acc: 0.7054 - val_loss: 0.7360 - val_acc: 0.6967\n",
      "Epoch 250/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7277 - acc: 0.7065 - val_loss: 0.7332 - val_acc: 0.6983\n",
      "Epoch 251/1000\n",
      "5275/5275 [==============================] - 0s 76us/sample - loss: 0.7272 - acc: 0.7065 - val_loss: 0.7380 - val_acc: 0.6990\n",
      "Epoch 252/1000\n",
      "5275/5275 [==============================] - 0s 78us/sample - loss: 0.7264 - acc: 0.7043 - val_loss: 0.7368 - val_acc: 0.6983\n",
      "Epoch 253/1000\n",
      "5275/5275 [==============================] - 0s 75us/sample - loss: 0.7270 - acc: 0.7062 - val_loss: 0.7324 - val_acc: 0.6998\n",
      "Epoch 254/1000\n",
      "5275/5275 [==============================] - 0s 79us/sample - loss: 0.7266 - acc: 0.7039 - val_loss: 0.7390 - val_acc: 0.7013\n",
      "Epoch 255/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7266 - acc: 0.7077 - val_loss: 0.7356 - val_acc: 0.6952\n",
      "Epoch 256/1000\n",
      "5275/5275 [==============================] - 0s 74us/sample - loss: 0.7259 - acc: 0.7046 - val_loss: 0.7349 - val_acc: 0.6967\n",
      "Epoch 257/1000\n",
      "5275/5275 [==============================] - 0s 83us/sample - loss: 0.7260 - acc: 0.7022 - val_loss: 0.7335 - val_acc: 0.6983\n",
      "Epoch 258/1000\n",
      "5275/5275 [==============================] - 0s 78us/sample - loss: 0.7254 - acc: 0.7062 - val_loss: 0.7322 - val_acc: 0.7013\n",
      "Epoch 259/1000\n",
      "5275/5275 [==============================] - 0s 76us/sample - loss: 0.7250 - acc: 0.7056 - val_loss: 0.7306 - val_acc: 0.6983\n",
      "Epoch 260/1000\n",
      "5275/5275 [==============================] - 0s 72us/sample - loss: 0.7255 - acc: 0.7054 - val_loss: 0.7343 - val_acc: 0.7013\n",
      "Epoch 261/1000\n",
      "5275/5275 [==============================] - 0s 76us/sample - loss: 0.7247 - acc: 0.7048 - val_loss: 0.7332 - val_acc: 0.6967\n",
      "Epoch 262/1000\n",
      "5275/5275 [==============================] - 0s 77us/sample - loss: 0.7245 - acc: 0.7026 - val_loss: 0.7319 - val_acc: 0.7066\n",
      "Epoch 263/1000\n",
      "5275/5275 [==============================] - 0s 70us/sample - loss: 0.7252 - acc: 0.7079 - val_loss: 0.7324 - val_acc: 0.6990\n",
      "Epoch 264/1000\n",
      "1920/5275 [=========>....................] - ETA: 0s - loss: 0.7202 - acc: 0.7172"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9dec8cdb3d75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 batch_size=32)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_epochs = int(1000)\n",
    "model.fit(new_data, new_label,\n",
    "                validation_split=0.2,\n",
    "                epochs=total_epochs,\n",
    "                batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 -> 2\n",
      "0.5 -> 3\n",
      "0.6 -> 6\n",
      "0.7 -> 7\n"
     ]
    }
   ],
   "source": [
    "for test in [0.4, 0.5, 0.6, 0.7]:\n",
    "    var = model.predict([test])\n",
    "    print(str(test) +\" -> \" + str(np.argmax(var)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
